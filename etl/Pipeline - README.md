# ğŸ§© ETL â€“ Inventario POS

## ğŸ“˜ DescripciÃ³n general
Este pipeline integra y transforma datos de inventario provenientes de mÃºltiples fuentes locales (archivos Excel y CSV), garantizando la **calidad, consistencia y unificaciÃ³n** de la informaciÃ³n antes de su carga en la base de datos.

El objetivo principal es disponer de un **proceso automatizado** que permita consolidar los inventarios de los distintos puntos de venta (POS) en una sola fuente confiable.

---

## ğŸ“¥ ExtracciÃ³n de datos

### Fuentes de entrada
Los datos provienen de diferentes archivos almacenados en el directorio `data/raw/`:

- `Inventario POS 1.xlsx`
- `Inventario POS 2.xlsx`
- `Inventario_3.csv`

El proceso detecta el tipo de archivo y aplica el mÃ©todo de lectura adecuado:

```python
for p in rutas:
    if p.endswith(".csv"):
        df = pd.read_csv(p, sep=",", encoding="utf-8-sig")
    elif p.endswith(".xlsx"):
        df = pd.read_excel(p)
    dfs.append(df)
```

Finalmente, todos los `DataFrames` se concatenan en uno solo para su posterior limpieza:

```python
df_final = pd.concat(dfs, ignore_index=True)
```

---

## ğŸ§¹ Limpieza y estandarizaciÃ³n

### Reglas aplicadas
1. **NormalizaciÃ³n de texto:**
   - ConversiÃ³n a minÃºsculas.
   - EliminaciÃ³n de acentos, caracteres especiales y espacios innecesarios.
   - SustituciÃ³n de valores nulos por cadenas vacÃ­as.

   ```python
   def limpiar_texto(txt):
       if pd.isna(txt):
           return ""
       txt = str(txt)
       txt = unicodedata.normalize("NFKD", txt).encode("ascii", "ignore").decode("ascii")
       txt = re.sub(r"\s+", " ", txt).strip().lower()
       return txt
   ```

2. **HomogeneizaciÃ³n de nombres de columnas:**
   - Todas las columnas se normalizan con nombres en minÃºsculas y sin espacios.
   - Se renombran campos crÃ­ticos para mantener coherencia entre las fuentes.

3. **EliminaciÃ³n de duplicados:**
   - Se eliminan registros repetidos segÃºn claves como `codigo_producto` o `descripcion`.

4. **Filtrado de registros vacÃ­os:**
   - Se descartan filas donde faltan datos esenciales como `cantidad` o `precio_unitario`.

5. **ConversiÃ³n de tipos de datos:**
   - Se asegura que los campos numÃ©ricos y de fecha tengan el formato correcto (`float`, `datetime`, etc.).

---

## âš™ï¸ Transformaciones adicionales

Durante la transformaciÃ³n, se realizan pasos clave para enriquecer el dataset:

- CÃ¡lculo de **campos derivados** (por ejemplo, `total_inventario = cantidad * precio_unitario`).
- Limpieza de **valores atÃ­picos** (por ejemplo, precios negativos o cantidades fuera de rango).
- Reordenamiento de columnas para mantener una estructura lÃ³gica.

Ejemplo:

```python
df_final["total_inventario"] = df_final["cantidad"] * df_final["precio_unitario"]
df_final = df_final[df_final["cantidad"] > 0]
```

Estas reglas aseguran que los datos cargados sean **coherentes, comparables y analÃ­ticamente Ãºtiles**.

---

## ğŸ’¾ Carga de datos

### MÃ©todo: **Upsert (inserciÃ³n o actualizaciÃ³n masiva)**

La funciÃ³n `bulk_upsert_inventario()` realiza una **carga masiva con detecciÃ³n de duplicados**.  
Esto significa que si un registro ya existe en la base de datos, se actualiza; si no, se inserta.

```python
from etl.db import bulk_upsert_inventario
bulk_upsert_inventario(df_final)
```

#### JustificaciÃ³n del mÃ©todo:
- **Eficiencia:** se insertan o actualizan miles de registros en bloque, reduciendo tiempos de ejecuciÃ³n.
- **Consistencia:** evita duplicar inventarios previamente cargados.
- **Escalabilidad:** permite ejecutar el pipeline de forma periÃ³dica sin limpiar la tabla completa.

---

## ğŸ§  Registro y manejo de errores

- Se usa `logger_setup.py` para capturar eventos, errores y tiempos de ejecuciÃ³n.
- Los errores en lectura o carga se manejan con bloques `try-except`, registrando el detalle en el log.
- Esto permite monitorear el pipeline y detectar problemas sin interrumpir la ejecuciÃ³n.

---

## ğŸ—‚ï¸ Estructura del proyecto

```
ETL_Sistemas_Seguridad/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Archivos de entrada
â”‚   â””â”€â”€ processed/          # Archivos limpios o transformados
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ logger_setup.py     # ConfiguraciÃ³n de logs
â”‚   â”œâ”€â”€ db.py               # ConexiÃ³n y operaciones con la base de datos
â”‚   â””â”€â”€ etl_inventario.py   # LÃ³gica principal del pipeline
â”‚
â”œâ”€â”€ main.py                 # Script de ejecuciÃ³n del ETL
â””â”€â”€ README.md               # DocumentaciÃ³n del pipeline
```

---

## ğŸš€ EjecuciÃ³n

1. Activar entorno virtual (si aplica):
   ```bash
   .venv\Scripts\activate     # En Windows
   source .venv/bin/activate  # En Linux/Mac
   ```

2. Ejecutar el script principal:
   ```bash
   python main.py
   ```

3. Verificar los logs en la carpeta `logs/` para confirmar la correcta ejecuciÃ³n del proceso.

---

## ğŸ“Š Diagrama de flujo del pipeline

```mermaid
flowchart TD
    A[ExtracciÃ³n de mÃºltiples fuentes<br>(Excel, CSV)] --> B[Limpieza de texto y columnas]
    B --> C[Transformaciones<br>(cÃ¡lculos, tipos, filtros)]
    C --> D[UnificaciÃ³n de DataFrames]
    D --> E[ValidaciÃ³n de calidad de datos]
    E --> F[Upsert en base de datos]
    F --> G[Logs y monitoreo de errores]
```

---
## â“ Soporte y guÃ­a adicional

En caso de tener dudas sobre cÃ³mo ejecutar correctamente el pipeline, refiÃ©rase al siguiente documento con las instrucciones detalladas:

ğŸ‘‰ [GuÃ­a oficial para correr el pipeline y la base de datos (PDF)](https://github.com/KrisSo03/ETL_Sistemas_Seguridad/blob/aff5f7c0229186f95e516249f0c353d94b0ef94f/etl/Proyecto%20-%20Intrucciones%20para%20correr%20pipeline%20y%20BD.pdf)

Este documento explica paso a paso la configuraciÃ³n del entorno, la conexiÃ³n a la base de datos y la ejecuciÃ³n completa del proceso ETL.


---
## ğŸ§¾ AutorÃ­a

**Proyecto ETL de Inventario POS**  
VersiÃ³n 1.0 â€” Noviembre 2025
